
So this is what happens in the app, a websocket connection is established between frontend and backend. The react frontend sends the audio sang by the user in realtime in chunks to the backend python fastapi app. 

This is how the app is in the frontend - similar to the uploaded image, the song selected by the user will appear and start playing and the pitch vs timelength graph appears and there is a pointer of sorts which is visible a vertical white line in the image that indicates the current point in the song. The Y axis is the pitch where as the X-axis is the timelength of the song note/word/etc . The orange color bars that you see are the pitch and timelength of the selected song, on top of this the pitch of singing user must also be shown. So the audio sent by frontend to backend must be analysed in realtime and the pitch analysed by backend must be shared to frontend and the frontend must display this information by comparing it with the pitch of the selected song at the current point in time where the song is playing. The vertical white line in the image represents the current point in time and the line itself is stationary. Its the bars that are there that represent the song, they keep moving as the song progresses. 

The pitch of the selected (previously analysed song ) and the audio that is sung by the user must be compared based on time i think because this is a literal comparison based on time and not lyrics. 

So write the code for this application, write code for frontend react and backend python fastapi. Also let me know the packages and libraries and models i need to download

Dont assume anything, if you have any doubts then ask me. 

-----------------
Now, i want to build an ai singing teacher agent. The problem sections found by the frontend must be sent to the backend and the backend must take these values
and then send the to an llm model along with a system prompt. This system prompt will contain instructions for the ai agent to give appropriate feedback to the user on the problem sections. The 
response from the llm model must be sent to a tts model which will then speak the feedback to the user. Now, i want an ai avatar to be shown in the frontend which will speak the feedback to the user.
This avatar must be a real human like avatar which will animate while speaking like a real human. 

Brainstorm with me on what to do and how to build this feature in the present codebase. 
